{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "master_df = pd.read_pickle(\"master_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its too much data for the computer to hold on ram, and thus it slows it down. To solve this we decided to save small chunks in hard memory to afterwards join it all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "e = 0\n",
    "a = len(master_df)\n",
    "for i in range(0,a,1000):\n",
    "    players_df = pd.DataFrame()\n",
    "    if(i+1000 > a):\n",
    "        for participant in master_df['participants'][i:a]:\n",
    "            e += 1\n",
    "            jsoned = json_normalize(participant)\n",
    "            players_df = players_df.append(jsoned)\n",
    "            print(format(e)+\"/\"+format(a))\n",
    "    else:\n",
    "        for participant in master_df['participants'][i:i+1000]:\n",
    "            e += 1\n",
    "            jsoned = json_normalize(participant)\n",
    "            players_df = players_df.append(jsoned)\n",
    "            print(format(e)+\"/\"+format(a))\n",
    "    players_df.to_pickle(\"players\"+format(int(i/1000))+\".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we load all chunks into a big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_df = pd.DataFrame()\n",
    "for i in range(0,a,1000):\n",
    "    players_df = players_df.append(pd.read_pickle(\"players\"+format(int(i/1000))+\".pkl\"))\n",
    "    print(format(int(i/1000)))\n",
    "players_df.to_pickle(\"players_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
